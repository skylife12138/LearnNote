## 网络模型：
- 1.select;  
- 2.iocp;    --proactor模式  
- 3.epoll;   --reactor模式  
- 4.kqueue;  --reactor模式   

## 开源网络库：  
- 1.ACE;  
- 2.ASIO;  
- 3.Libevent;  

## IOCP:完成端口
- 1.初始化work线程池,线程数量为cpu数量*2,监听和客户端连接线程都从work线程中分配,并为每个线程创建一个完成端口；  
- 2.从work线程池中分配一个监听线程,执行基本的绑定监听流程,同时监听128个连接,并将监听端口和监听线程的完成端口绑定，然后投递128个AcceptEx；  
- 3.当有客户端请求连接服务器时,监听线程执行 **OnAccept**，在OnAccept中创建连接端口,并为其从work线程中分配并绑定一个线程,将连接端口与绑定的线程中的完成端口绑定,并向连接端口投递 **WSARecv**;  
- 4.然后这个连接的客户端就会在其连接端口分配的线程上与服务器进行通信,包括发送接收消息,消息统一都是完成端口返回,通过switch不同的类型进行不同的操作；  

注:1.完成端口是绑定在监听端口上的;  

## Epoll:
- 1.初始化work线程池，线程输了为cup数量*2，监听和客户端连接线程都从work线程分配，每个线程调用 **epoll_create(maxscoke)**  函数监听maxscoke数量的端口；  
- 2.监听端口进行完绑定等操作后会分配到一个工作线程，该线程会调用**epoll_ctl()**添加(EPOLL_CTL_ADD)文件描述符(端口),其中网络事件为EPOLLIN；
- 3.当有客户端连接或者通讯时, **epoll_wait()**函数会获取内核就绪的文件描述符，并根据其网络事件类型进行不同的处理,处理完后将该端口的网络事件change成初始状态，如EPOLLOUT或EPOLLIN，以便在内核中继续等待网络事件进行处理；

##### IOCP为windows特有的网络模型，该模型在通知应用程序网络事件时内核已经完成了消息的拷贝，所以为非阻塞的；
##### Epoll为linux特有的网络模型，该模型是在内核中维护着一个红黑树，保存着需要等待消息的文件描述符，并为每个注册的文件描述符添加了一个回调函数，如果有网络事件到来，对应文件描述符调用回调函数通知应用程序，然后应用程序获取网络消息，这块使用了共享内存mmap来保证不用拷贝消息达到高效；
##### IOCP和Epoll都适合于高并发的网络环境，而select和poll则因为采用了轮询机制，每次都要将所有等待端口遍历一次来查找已经就绪的网络事件，导致其效率不高；

### libzmq 和 CrossEngine对比：
   CrossEngin将

### libevent:
- 1.bufferevent的可读高水位和低水位的含义：
  - 低水位是指当evbuffer缓冲区的可读数据量小于该数值时不会调用用户的读回调函数；
  - 高水位是值当evbuffer缓冲区的可读数据量大于该数值时挂起可读事件，读缓冲区不会从socket中继续读取数据，也避免了因为socket中有可读数据导致一直触发监听读的event形成的死循环。这样做限制的evbuffer缓冲区的大小，只有当evbuffer中的数据量小于高水位值时才会恢复挂起的可读事件继续读取socket的数据；
- 2.bufferevent的读监听和写监听的区别：
  - 读监听是指当监听读事件的event检测到socket的读缓冲区有数据时就认为可读，即触发读回调函数；
  - 写监听不能用监听写事件的event检测socket的写缓冲区未满时做为可写,因为大部分时间写缓冲区都不满，导致该event一直触发写回调函数，造成死循环。libevent中只有在真正写时调用函数bufferevent_write()函数时，才会将该event添加(event_add)到event_base中进行监听，而当所有数据都写入到socket的写缓冲区后就删除该event,避免死循环；

### ZMQ问题：

ZMQ库zmq_recv()函数返回的是字符串，无法获取发送端的ip和端口等信息，如果作为客户端和服务器的通讯库，则需要在上层分配每个客户端的唯一编号；

改进方案：修改ZMQ库代码，精简定制功能，